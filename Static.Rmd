---
title: "Evolutionary Game Theory"
author: "Sebastian Schreiber"
date: Started July 7, 2015; Last Update July 22, 2015
output: html_document
---

These notes are the basis of the lectures that I give as part of the "Mathematical Modeling of Biological Systems" Cluster of the <a href="cosmos.ucdavis.edu">UC Davis COSMOS program.</a> This cluster is currently run by Tim Lewis, Bob Guy, and myself. My lectures cover evolutionary game theory and disease dynamics. 


# Introduction

Populations of organisms, whether they be plants, animals, or viruses, consist of individuals with different genotypes - the blueprint inherited from their parents which, along with their environment, determines their behavior, physiology, and morphology. At any point in time, individuals of one of these genotypes may be better at dealing with the current environment and, consequently, live longer and produce more offspring than individuals with other genotypes. When this occurs, this more "fit" genotype will increase in frequency relative to the other genotypes. This combination of offspring inheriting their parental genotypes and survival of the fittest is the basis of evolution by natural selection as described by Charles Darwin at the end of the 19th century. This simple, yet profound principle has provided exceptionally useful in understanding the origins of diversity, combat diseases, and  produce food more efficiently. 

Evolutionary games provide a simple framework to mathematically encode key element of evolution by natural selection. This framework allows one to tackle a variety of fundamental biological questions such as: If natural selection favors more "fit" individuals, how does non-aggressive behavior and, more shockingly, altruistic behavior evolve? Why do many species produce equal ratios of sons and daughters? *more to come* In these lecture notes, I will introduce the fundamentals of evolutionary game theory and its applications to these and other questions. 



# Static Games

Even though evolution by natural selection is a dynamic process, we will begin by examining static games which provided the historical context for discussing dynamic games. These games envision a two players that interact by each playing one of a finite number of strategies. The outcome of these interactions provide a payoff to each player which contribute to an individual's lifetime reproductive success. To start the discussion, we consider a classic game, the prisoner's dilemma, which involves a contest between two humans. 

Envision that you and a friend have committed a major robbery and have been arrested. If convicted of this crime, you will spend $30$ years in prison. Fortunately for you, the police have only sufficient evidence to convict either of you of a much lesser crime, carrying a unlicensed firearm. Convicted of this crime, you would only spend $5$ years in prison. In the hope to extract a confession from one of you, the police have placed you in separate interrogation rooms. In each room, they make the following offer "If you provide evidence against your partner on the robbery but your partner does not provide evidence against you, you will go free and your partner will do the time. If you both provide evidence against each other, you will both do the time but with a reduced sentence of $15$ years."  **What do you do?** 

One approach to answering this question is treat the problem as a *two player* (you and your partner), *symmetric* (you are both offered the same deal) *game*. As you have one of two choices, *defect* on your partner or *cooperate* with your partner, the game is a *two strategy* game. Associated with each pair of strategies played by you and your partner is a *payoff* for each you i.e. how much you gain or lose when this pair of strategies is played. In the case of our prisoner's dilemma, if you and your partner cooperate (i.e. don't rat one another out), each of you "only" lose $5$ years of their life; the payoff to each of you is $-5$. Alternatively, if both of you defect, each you gets convicted of the major crime with a reduced sentence and get payoffs of $-15$. Finally, if one defects and the other cooperates, the defector gets a payoff of $0$ and the cooperator gets a payoff of $-30$. 

We can summarize this information in a *payoff matrix* as follows:

 ||Cooperate|Defect
---|---------|------
Cooperate|-5|-30
Defect|0|-15

where rows correspond to the strategies of the focal player and columns correspond to the strategies of the opponent. 

**If you want to maximize your payoff (i.e. years of prison free life), what is the best strategy if your opponent defects? cooperates?** Clearly, in both cases, the best strategy is to defect. Hence, the dilemma.

We can generalize this game by allowing for arbitrary pay-offs. Say there are two strategies, strategy $1$ and strategy $2$. Let $a_{11}$ be the payoff to an individual playing strategy $1$ against an individual playing strategy $1$, $a_{12}$ be the payoff to an individual playing strategy $1$ against an individual playing strategy $2$, $a_{21}$ be the payoff to an individual playing strategy $2$ against an individual playing strategy $1$, etc. Then the payoff matrix becomes  

 ||Strategy $1$|Strategy $2$
---|---------|------
Strategy $1$|$a_{11}$|$a_{12}$
Strategy $2$|$a_{21}$|$a_{22}$

In the case of the prisoner's dilemma, we had $a_{11}=-5$, $a_{12}=-30$,$a_{21}=0$, and $a_{22}=-15$. In this game, there was a strategy (defect) which always performed better than the other strategy. We call such a strategy, a *dominant strategy*. More generally, strategy $2$ (respectively $1$) is a *dominant* strategy if $a_{21}>a_{11}$ and $a_{22}>a_{12}$ (respectively $a_{21}<a_{11}$ and $a_{22}<a_{12}$).

Do all games have a dominant strategy? Consider  the **hawk-dove** game. 

### the Hawk-Dove game

The hawk dove game was introduced by John Maynard-Smith to study the evolution of aggression in animal contests. About this game, Richard Dawkins wrote (*Selfish Gene* 1976): 

>Survival machines of the same species tend to impinge on each others'
lives more directly. This is for many reasons. One is that half the
population of one's own species may be potential mates, and potentially
hard-working and exploitable parents to one's children. Another reason
is that members of the same species, being very similar to each other,being machines for preserving genes in the same kind of place, with the
same kind of way of life, are particularly direct competitors for all the
resources necessary for life. To a blackbird, a mole may be a competitor,
but it is not nearly so important a competitor as another blackbird.
Moles and blackbirds may compete for worms, but blackbirds and
blackbirds compete with each other for worms and for everything else...The logical policy for a survival machine might therefore seem to be to
murder its rivals, and then, preferably, to eat them. Although murder
and cannibalism do occur in nature, they are not as common as a naive
interpretation of the selfish gene theory might predict. Indeed Konrad
Lorenz, in On Aggression, stresses the restrained and gentlemanly
nature of animal fighting. For him the notable thing about animal fights
is that they are formal tournaments, played according to rules like those
of boxing or fencing. Animals fight with gloved fists and blunted foils.
Threat and bluff take the place of deadly earnest. Gestures of surrender
are recognized by victors, who then refrain from dealing the killing blow
or bite that our naive theory might predict...Why is it that animals do not go all out to
kill rival members of their species at every possible opportunity?

To resolve this apparent paradox, we consider describe a particular instance of Maynard Smith's hawk-dove game. In this game, imagine that individuals engage in contests over a limiting resource e.g. a potential mate, food, shelter. Lets say  receiving this resource will increase one's offspring production by $2$ offspring. Individuals can play one of two strategies, hawk or dove. Hawk strategists always escalate the interaction until getting hurt or winning the entire resource. If one gets hurt, offspring production is reduced by $3$. Dove strategists run when there is an escalation and otherwise share the resource. **What is the payoff matrix for this game?**

 ||Dove|Hawk
---|---------|------
Dove|$1$|$0$
Hawk|$2$|$\frac{2-3}{2}=-\frac{1}{2}$

where the payoff from the hawk-hawk interaction corresponds to the average outcome: winning half of the time, losing half of the time. 

**Does this game have a dominant strategy? If not, why not?** Well, if your opponent plays dove, the best strategy is to play hawk. Alternatively, if your opponent plays hawk, the best strategy is to play dove. However, as you won't know what your opponent plays in advance, is there any notion of a "best strategy" for this game? 

To answer this question, we need to introduce a larger class of strategies, so-called *mixed strategies*. Mixed strategies are probabilistic strategies in which an individual plays one strategy with some likelihood and the other strategy with the complementary likelihood. For example, my opponent might roll a six-sided die to determine their strategy: playing dove if they get $1$, and hawk otherwise. 

The outcome of playing the game once against this opponent is stochastic as it depends on the roll of the die. For example, suppose I choose to play dove. There is a $1/6$ chance that I get a reward of $+1$ and a $5/6$ chance that I get no reward. Now suppose I played the game many times against this opponent. **What does the average payoff look like?** Recall, the average of a set of numbers, say $x_1,...,x_n$, is given by dividing their sum by the number of numbers i.e. $\frac{x_1+\dots+x_n}{n}$. Using the computational package R, we can simulate playing this game against the mixed strategist $n$ times and compute the average payoff as follows.
```{r,fig.width=4,fig.height=4}
n=1000 # number of times playing as dove
outcomes=sample(x = 1:6,size = n,replace = TRUE) # roll the die n times 
payoffs=(outcomes==1)*1+(outcomes>1)*0  #payoff of 1 when opponent plays dove, 0 otherwise
hist(payoffs,breaks = c(-0.5,0.5,1.5),col="red") # histogram of payoffs
mean(payoffs) # computes the average (mean)
```
**Try running this several times for $n=5,10,100,1000,10000,100000$. What do you notice?** As $n$ gets larger, there is less variation in the mean payoff across runs, and the mean payoff appears to stabilize around the value $1/6$. **What does this $1/6$ represent?** Well, if we play dove many times against our opponent, roughly $1/6$ of time we get a payoff of $1$ and $5/6$ of the time we get a payoff of $0$. Hence, we expect a payoff of 
\[
\frac{1}{6}\times 1 + \frac{5}{6}\times 0 = \frac{1}{6}
\]
This quantity is called *the expected payoff* of this dove strategy against the $\frac{1}{6}$ strategist. One can visualize the convergence to the expected payoff by plotting the average payoff as a function of the number of games:
```{r,fig.width=6,fig.height=4}
plot(1:n,cumsum(payoffs)/1:n,typ="l",bty="n",lwd=3,xlab="number of games",ylab="mean payoff")
abline(h=1/6,col="red")
```


**What is the expected payoff if I play hawk?** Simply,
\[
\frac{1}{6}\times 2+ \frac{5}{6}\times\left(-\frac{1}{2}\right)=-\frac{1}{12}
\]
as we can demonstrate numerically by playing a 10,000 times against the computer
```{r,fig.width=6,fig.height=4} 
n=10000 # number of times playing hawk
outcomes=sample(x = 1:6,size = n,replace = TRUE)
payoffs=(outcomes==1)*2+(outcomes>1)*(-1/2)
plot(1:n,cumsum(payoffs)/1:n,typ="l",bty="n",lwd=3,xlab="number of games",ylab="mean payoff")
abline(h=-1/12,col="red")
```
Despite playing $10,000$ times, we didn't precisely get $-\frac{1}{12}$. This is to be expected as we didn't play an infinite number of times. How likely our particular outcome was can be estimated using the *Central Limit Theorem* (check it out on Wikipedia)

More generally, if our opponent plays strategy $1$ with probability $x$ and strategy $2$ with probability $1-x$, then my expected payoff playing as strategy $1$ is 
\[
p_1(x)=x a_{11}+(1-x)a_{12}
\]
and my expected payoff playing as strategy $2$ is 
\[
p_2(x)=x a_{21}+(1-x)a_{22}
\]
**What are these in terms of the hawk-dove game?** Simply, 
\[
p_1(x)=x\times 1+(1-x)\times 0 = x
\]
and 
\[
p_2(x)=x \times 2 + (1-x)\times (-1/2)=\frac{5}{2}x-\frac{1}{2}
\]

Now suppose, your opponent plays dove with probability $x$ but you play dove with probability $y$. **What is your expected payofff?** When you play dove, you get $p_1(x)$, otherwise $p_2(x)$. As you plays dove a fraction $y$ of the time and hawk otherwise, the expected payoff is 
\[
p(x,y)=\mbox{payoff playing $y$ against $x$}=
yp_1(x)+(1-y)p_2(x)
\]

To make sure you "get" this definition, **what is expected payoff when you play dove one-half of the time and your opponnent plays dove one-sixth of the time?** We have $p_1(1/6)=1/6$ and $p_2(1/6)=-1/12$ from our earlier calculations. As $y=1/2$, we get
\[
p(1/6,1/2)=\frac{1}{2}\times \frac{1}{6}+\frac{1}{2}\frac{-1}{12}=\frac{1}{24}
\]
Simulating this interaction $1,000,000$ times supports our calculations
```{r}
n=1000000 # number of times playing against the dove
outcomes1=sample(x = 1:6,size = n,replace = TRUE)
outcomes2=sample(x = 1:6,size = n,replace = TRUE)
payoffs=(outcomes1==1&outcomes2<4)*1+(outcomes1==1&outcomes2>3)*2+(outcomes1>1&outcomes2<4)*0+(outcomes1>1&outcomes2>3)*(-0.5)
mean(payoffs)
```

##Nash equilibria

OK. So have a bunch of new strategies...but is one of them the "best"?  In his Nobel prize winning work, John Nash introduced a concept of a "best" strategy and showed such a strategy always exists for finite player, finite strategy games. (<span style="color:blue">Show clips from <a href="https://en.wikipedia.org/wiki/A_Beautiful_Mind_(film)">A Beautiful Mind</a></span>)

**Definition** A strategy $x$ is a *Nash equilibrium* if $p(x,x)\ge p(x,y)$ for all $y$ in $[0,1]$. In words, your opponent's strategy is a Nash equilibrium if one can not get a larger payoff playing any other strategy against them. While, as we shall soon seen, this definition isn't completely satisfying for characterizing evolutionary end points or "best" strategies, this condition certainly seems like a necessary condition for strategies corresponding to evolutionary end points.  

How can we find such a strategy? Notice that  $p(x,y)$ is a linear function of $y$. Hence, if $0<x<1$ is a Nash equilibrium, then $p(x,y)$ as a function $y$ must be constant else either $x<y$ or $x>y$ yields a higher payoff $p(x,y)$ than $p(x,x)$. But, this is only possible if $p_1(x)=p_2(x)$. Thus we have shown *A mixed strategy $0<x<1$ is a Nash equilibrium if and only if $p_1(x)=p_2(x)$.* Alternatively,  a "pure" strategy $x=0$ or $x=1$ is a Nash equilibrium if playing the other strategy against it doesn't lead to a higher payoff. **Check this yourself!**

**What are the Nash equilibria for the Hawk-Game?** We have already seen that the pure strategies are not Nash equilibria (i.e. playing the other strategy leads to a higher payoff). Any mixed Nash equilibrium must satisfy $p_1(x)=p_2(x)$ which yields $x=(5/2)x-1/2$ i.e. $x=1/3$. Playing dove one-third of the time yields an unbeatable strategy. The expected payoff when both players play this strategy is $p_1(1/3)=1/3$. Hence, the "optimal" strategy involves a mixture of aggression and passive behavior. But is this always the case? When is it "better" to exhibit less aggression? 

To answer the preceding questions, we can generalize the hawk-dove game by letting $V$ (for victory) denote the value of the resource and $C$ (for cost) denote the value of the cost. The hawk-dove payoff matrix becomes: 

||Dove|Hawk
---|---------|------
Dove|$V/2$|$0$
Hawk|$V$|$\frac{V-C}{2}$

**Try answering the following questions by solving the Nash equilibria:**

1. Under what conditions is there a pure Nash equilibrium? 
2. When does the Nash equilibrium correspond to less aggressive behavior? 

**Answers:** 

1. If $V\ge C$, then hawk is a dominant strategy. In particular, pure hawk is a Nash equilibrium. If $V<C$, then there is no pure Nash equilibrium. In particular, being purely dove is never the "best" strategy. 

2. If $C>V$, then there is only one Nash equilibrium given by 
\[
1-\frac{V}{C}
\]
Hence, the Nash equilibrium corresponds to less aggressive behavior for larger cost to victory ratios. 


## Evolution of cooperation and the TFT strategy

>Cooperation in organisms, whether bacteria or primates, has been a difficulty for evolutionary theory since Darwin. On the assumption that interactions between pairs of individuals occur on a probabilistic basis, a model is developed based on the concept of an evolutionarily stable strategy in the context of the Prisoner's Dilemma game. Deductions from the model, and the results of a computer tournaments how how cooperation based on reciprocity can get started in an asocial world, can thrive while interacting with a wide range of other strategies, and can resist invasion once fully established. Potential applications include specific aspects of territoriality, mating, and disease. - Robert Axelrod and Bill Hamilton (1981) *Science*


Lets considered a repeated version of the prisoner's dilemma. Assume that individuals can either cooperate or defect. You receive a benefit $B$ when you opponent cooperates with you and pay a cost $C$ for cooperating with your opponent. Defectors only receive the benefit and pay no cost. We have the following payoff matrix

||Cooperate|Defect
---|---------|------
Cooperate|B-C|-C
Defect|B|0

This leads to a prisoner's dilemma where it is always best to defect. But lets suppose the game is iterated $n$ between pairs of individuals, and there are two strategies: tit-for-tat and always defect. The defector always fails to cooperator. The tit-for-tatter always starts with cooperating but then plays what their opponent played in the prior round of the game. **How does the pay-off matrix change?**

||Tit for Tat |Defect
---|---------|------
Tit for Tat|n(B-C)|-C
Defect|B|0

**Under what conditions is there no dominant strategy?** Since defecting is the best strategy against a defector, we need TFT is the best strategy against itself. This requires $n(B-C)>B$ or equivalently
\[
\frac{n-1}{n}B>C
\]. In particular, need $B>C$ and $n$ to be sufficiently larger than $1$. Suppose this condition holds. **Is there a Nash equilibrium?** Each of the pure strategies is a Nash equilibrium. Hence cooperation is possible provided condition above holds. 

<span style="color:blue"> Discuss vampire bat work of Gerald Wilkinson (1984) in *Nature* </span>

**What about mixed Nash equilibria?** A mixed Nash equilibrium must satisfy
\[
\begin{aligned}
xn(B-C)+(1-x)(-C)&=xB\\
x(n-1)(B-C)&=C\\
x=&\frac{C}{(n-1)(B-C)}
\end{aligned}
\]
Hence, they exist as well. So what does this mean? Which strategy may evolve? To answer these questions, we make the games dynamic. 

**SKIPPED THE DEFINITION BELOW as the concept of stability is clearer in the dynamic section.**
John Maynard-Smith and George Price introduced the concept of an evolutionary stable strategy. Intuitively, an evolutionary stable strategy is a strategy when played by most of the population can not be "invaded" by any other strategy. Formally, their condition states $x$ is an *evolutionarily stable strategy* if either 

1. $p(x,x)>p(x,y)$ OR
2. $p(x,x)=p(x,y)$ and $p(y,x)>p(y,y)$

for all $y\neq x$. The first condition asserts that in a population of individuals primarily playing strategy $x$, individuals playing strategy $x$ are more than individuals playing any other strategy $y$. The second condition asserts if another strategy has equal fitness, then individuals playing strategy $x$ yields a higher fitness in a population of individuals playing the other strategy. In other words, the frequency of individuals playing strategy $x$ would increase in any population playing a different strategy. 

To explore this definition, lets work through two examples with the hawk-dove game and the modified prisoner's dilemma. For the hawk-dove game, we found that $x=1/3$ is a Nash equilibrium satisfying $p(x,x)=p(x,y)$ for all $y$ in $[0,1]$. As this corresponds to the second case of the ESS definition, we also need to verify whether $p(y,1/3)>p(y,y)$ for all $y\neq 1/3$. *To be added!*

# Replicator Dynamics

The Nash equilibrium concepts assumes that the two players are *fully rationale* and *have complete knowledge* about the payoff matrix. However, we hardly can expect this to be the case for most individuals, whether we are talking humans or bacteria. <span style="color:blue">Play clip from <a href="https://en.wikipedia.org/wiki/The_Princess_Bride_(film)"> The princess bride </a>!!</span> Furthermore, our games have been static, but evolution is, by definition, a dynamic process in which frequencies of genotypes change over time. To account for this dynamic, we will model the dynamics of two competing genotypes. 

To keep things simple, we assume asexual reproduction whereby like begets like. Lets call these genotypes, genotype $1$ and genotype $2$. These genotypes live in an environment which can support only $N$ individuals (Here $N$ is XXL). Let $x_t$ be the frequency of genotype $1$ at time $t$. Let $p_i(x_t)$ be the number of offspring produced by genotype $i$ at time $t$. As the notation suggests, these $p_i$ may correspond to the payoff functions from the static models where the opponent is a randomly chosen individual from the population. 

To define the population dynamic, we assume that each time step (e.g. year) a fraction $\delta$ of individuals die, thereby freeing up territories for $\delta N$ individual offspring to settle. Each offspring (independent of genotype) is equally likely to get a freed up territory. **Armed with this information, what might the update rule be for $x_t$?**

Well, a bit of thought (*will add more later*) reveals
\[
x_{t+1}=(1-\delta) x_t + \delta \frac{p_1(x_t)x_t}{p_1(x_t)x_t+p_2(x_t)(1-x_t)}
\]
Here is some R code to simulate this model 
```{r}
replicator2=function(p1,p2,delta=0.001,x0=seq(0,1,length=15),T=5000){
  n=length(x0)
	x=matrix(0,T+1,n)
	x[1,]=x0
	for(t in 1:T)x[t+1,]=(1-delta)*x[t,]+delta*p1(x[t,])*x[t,]/(p1(x[t,])*x[t,]+p2(x[t,])*(1-x[t,]))
	par(cex.lab=1.5,cex.axis=1.5)
matplot(x,type="l",lty=1,lwd=2,xlab="time",ylab="frequency")
	}
```
The input for the replicator2 function is the p1 and p2 functions, the death rate, and the initial frequencies of interest. 

Lets try out this function for hawk-dove game with 
$p_1(x)=1+x$ and $p_2(x)=1+(5x-1)/2$ (Note: I added $1$ as the baseline payoff to insure no negative numbers).
```{r}
p1=function(x)1+x
p2=function(x)1+(5*x-1)/2
replicator2(p1,p2,delta=0.01,T=2000)
```

This simulation suggests that all initial conditions involving both genotypes approaches a value of $x=1/3$. In other words, in the long-term, one-third of the population will be dove genotypes and two-thirds will be hawk genotypes. Is it a coincidence that we are getting $1/3$ here? Certainly not! **To see why, determine what the equilibria of the replicator must satisfy.** A little thought reveals that $0$ and $1$ are always equilibria (i.e. the no cats, no kittens principle). Furthermore, $0<x<1$ is an equilibrium if and only if $p_1(x)=p_2(x)$. Namely, any polymorphic equilibrium corresponds to a Nash equilibrium! 

<span style="color:blue">Discuss</a> the <a href="http://biorxiv.org/content/early/2014/11/03/011049">Healey and Gore</a> paper on hawk-dove dynamics in yeast populations!</span>

In the hawk-dove game, we can see that the Nash equilibrium $x=1/3$ is stable for the replicator dynamics. This need not be true. To see why, lets consider the modified prisoner's dilemma model with $n=10$, $B=5$, $C=1$. In this case, $0$, $1$, and 

```{r}
C=1
B=5
n=2
C/((n-1)*(B-C))
```

are Nash equilibria. **What do you notice when you run the dynamics for this example?**

```{r}
p1=function(x)1+x*(n*(B-C))+(1-x)*(-C)
p2=function(x)1+x*B+(1-x)*0
replicator2(p1,p2,delta=0.01,T=2000)
```

The Nash equilibria $0,1$ are stable, but the Nash equilibrium of $1/3$ is unstable. Hence, there is convergence to the all cooperation state only if there are initially sufficiently many cooperators in the population. 

*How might one check for stability?* Provided that $\delta>0$ is sufficiently small, it is fairly simple. Say $0<x^*<1$ is a Nash equilibrium. This equilibrium will be stable for the dynamic provided that the frequency of strategy $1$ is increasing when it is slightly slightly below $x^*$, and decreasing when it is slightly above $x^*$. In equations **(Check this for yourselves!!)**, $p_1(x)>p_2(x)$ for $x$ slightly smaller than $x^*$ and $p_1(x)<p_2(x)$ for $x$ slightly larger than $x^*$. Things get even simpler for the $p_i(x)$ functions that we have been using as they are linear in $x$. Hence **(check this for yourselves!)**, the stability condition reduces to $p_1(0)>p_2(0)$ (strategy $1$ increases when rare) and $p_2(1)>p_1(1)$ (strategy $2$ increases when rare). Conversely, if any of the inequalities are reversed, $x^*$ is unstable. 

**Try this out for the hawk-dove game and modified prisoner's dilemma, and verify that the simulations are doing the right thing :)** In the hawk-dove game, we have $p_1(0)=1>1/2=p_2(0)$ and $p_2(1)=3>2=p_1(1)$. Hence, the equilibrium $x=1/3$ is stable, as we observed in the numerics. In the modified prisoner's dilemma, $p_1(0)=0<1=p_2(0)$. Hence, the equilibrium $x=1/4$ is unstable, as we observed in the numerics.

# Best-response dynamics

In light of the lab on Blepharisma population dynamics, I thought it would be fun to take a little detour to another type of game dynamics. As you saw, individual cells of Blepharisma (by forgoing some cell division events) could become "giant cannibals" which not only eat bacteria but fellow protozoans. This shift in morphology and behavior seems to be "plastic" i.e. individuals switch to this behavior only when there are many "dwarf" (i.e. non-giant) Blepharisma to consume. Cannibalism is not unique to Blepharisma as this <a href="https://www.youtube.com/watch?v=2frwxlzsPac"> clip </a> shows. 

To model these behavioral dynamics (in a simple way), let us assume that there is the base fitness for dwarfs is $a$ derived from eating bacteria, but the base fitness for the giants is $0$ as they only eat dwarfs (**Think about how to relax this extreme assumption**). When dwarfs encounter one another, there is no effect on their fitness. However, when a giant encounters a dwarf, it captures it with probability $p$ in which case gets $b$ offspring and the dwarf dies losing its $a$ offspring. When two giants encounter one another, there is no effect on their fitness. Hence, the payoff matrix is 

||Dwarf |Giant
---|---------|------
Dwarf|a|p(-a)+(1-p)a=a(1-2p)
Giant|pb|0

**When do you get a hawk-dove game? Coordination game?** To get a hawk-dove game, we need that $pb>a$ and $p<1/2$ i.e. eating dwarves has to be worthwhile, but dwarves have to have a reasonable chance of escaping. If $x$ is the frequency of dwarves in the population, then the payoff functions of interacting with a random individual from the population are given by 
\[
\begin{aligned}
p_1(x)&=a x+(1-2p)a(1-x)\\
p_2(x)&=pbx
\end{aligned}
\]
where $x$ is frequency of dwarfs in the population. 

To model the behavior dynamics of the Blepharisma, lets assume that individuals change their phenotype (i.e. dwarf vs. giant) if it will increase their fitness. In particular, a giant will become a dwarf if 
\[
\begin{aligned}
p_1(x)&>p_2(x)\\
a(1-p)+apx&>pbx\\
a(1-p)&>p(b-a)x\\
\frac{a(1-p)}{p(b-a)}&>x
\end{aligned}
\]
If the inequality is reversed, dwarfs will become giants. 

To model the changes in the frequency $x_t$ of dwarfs in the population due to shifts in behavior, lets assume only a fraction $\delta$ of the population updates per time step and they play the strategy that would maximize their fitness. If $p_2(x_t)<p_1(x_t)$, then all the updating individuals become/stay dwarves and
\[
x_{t+1}=(1-\delta)x_t+\delta 
\]
If $p_2(x_t)>p_1(x_t)$, then all the updating individuals become/stay giants and
\[
x_{t+1}=(1-\delta)x_t 
\]

The following code allows us to simulate these piece-wise linear dynamics: 

```{r}
best.response=function(p1,p2,delta=0.001,x0=seq(0,1,length=15),T=1500){
  n=length(x0)
	x=matrix(0,T+1,n)
	x[1,]=x0
	for(t in 1:T){
	  temp=x[t,]*(1-delta)
	  become1=which(p1(x[t,])>p2(x[t,]))
	  if(length(become1>0))temp[become1]=temp[become1]+delta
	  x[t+1,]=temp
	  }
	par(cex.lab=1.5,cex.axis=1.5)
matplot(x,type="l",lty=1,lwd=2,xlab="time",ylab="frequency")
	}
```

Lets try this out for parameter values satisfying $pb>a$ and $p<1/2$: 

```{r}
b=25;a=7;p=1/3
p1=function(x)a*x+a*(1-2*p)*(1-x)
p2=function(x)p*b*x
best.response(p1=p1,p2=p2)
```

Notice that the dynamics are converging to a special value at which the population consists of a mixture of giants and dwarfs. **What is this value? Is it a Nash equilibrium? When will giants be rare in the population?** Answer is partially shown in the code below. 

```{r}
b=25;a=7;p=1/3
p1=function(x)a*x+a*(1-2*p)*(1-x)
p2=function(x)p*b*x
best.response(p1=p1,p2=p2)
abline(h=a*(2*p-1)/(p*(2*a-b)),lty=2,lwd=4)
```

# Three or more genotypes

What about more than $2$ phenotype? Everything generalizes quite naturally. Before stating the general forms of the static games and replicator dynamics, lets consider the rock, paper, scissor game of childhood fame.  This game is characterize by an intransitivity: scissor beats paper, paper beats rock, and rock beats scissor. Remarkably, this game shows up in a variety of natural populations. <span style="color:blue"> Discuss and rock, paper, scissor in *E.coli* populations and side-blotched lizards.</span> 

```{r}
replicator3=function(p1,p2,p3,delta=0.01,T=15000,x0,y0,type=1){
  n=length(x0)
	x=matrix(0,T+1,n)
	y=matrix(0,T+1,n)
	x[1,]=x0
	y[1,]=y0
	for(i in 1:n){
	for(t in 1:T){
		xt=x[t,i]
		yt=y[t,i]
		temp1=p1(xt,yt)*xt
		temp2=p2(xt,yt)*yt
		temp3=p3(xt,yt)*(1-xt-yt)
		temp=temp1+temp2+temp3
		x[t+1,i]=(1-delta)*xt+delta*temp1/temp
		y[t+1,i]=(1-delta)*yt+delta*temp2/temp
	}
	}
	if(type==1){
		x.new=x+y/2
		y.new=y/1.14142
		par(cex.lab=1.5,cex.axis=1.5)
	matplot(x.new,y.new,type="l",lty=1,lwd=2,xlab="",ylab="",xaxt="n",yaxt="n",bty="n",xlim=c(-0.1,1.1),ylim=c(-0.1,1/1.14142+0.1))
		lines(c(0,1,1/2,0),c(0,0,1/1.14142,0),lwd=3)
		text(0,-0.05,"strategy 3",cex=1.25)	
		text(1,-0.05,"strategy 1",cex=1.25)	
		text(0.5,1/1.14142+0.05,"strategy 2",cex=1.25)
		}
	if(type==2){
			matplot(x,type="l",lwd=2,xlab="time",ylab="frequencies",ylim=c(0,1),col="blue")
		matplot(y,type="l",lwd=2,add=TRUE,col="red")
		matplot(1-x-y,type="l",lwd=2,add=TRUE,col="green")}	
}
```

rock-paper-scissor :)

```{r,fig.height=5,fig.width=5}
p1=function(x,y)2-y+(1-x-y)
p2=function(x,y)2+x-(1-x-y)
p3=function(x,y)2-x+y
replicator3(p1,p2,p3,x0=seq(0.1,0.3,length=10),y0=seq(0.1,0.3,length=10))
```

What about more strategies? **Play Big Bang <a href="https://www.youtube.com/watch?v=_PUEoDYpUyQ">clip</a> about rock-paper-scissor-lizard-spock** 

#Evolution in Stochastic Environments

#Population growth and the geometric mean

All populations, whether they consist of plants, animals, or viruses, experience variation in environmental conditions like precipitation, temperature, nutrient or resource availability, etc. As these environmental conditions influence the ability of individuals to survive and reproduce, fluctuations in the environmental conditions invariably lead to fluctuations in population sizes. 

Here, we examine the influence of these fluctuations by using a very simple model of population growth. Before stating the general version of this model, lets consider a fairly specific scenario. We have a population desert annuals i.e. plants that live one year and live in a desert. The reproductive success of these annuals depends on whether there it was a wet or dry year (yes deserts have wet years!). Lets say each year there is a 40\% chance it is dry, else it is wet. In a dry year, individual plants produce 25 seeds. In a wet year, individual plants produce 200 seeds. In all years, 1\% of seeds successfully germinate and remaining 99\% are lost to seed predation, landing in a poor environment, or getting washed out of the system. **So what should our model be?** 

**Answer:** On wet years, each plant produces $0.01\times 200=2$ individuals that replace it in the next year. In dry years, on average plants produce $0.01 \times 25=0.25$ individuals to replace themselves e.g. one out of four plants successfully replace themselves. Hence, the model becomes
$x_{t+1}= 2x_t$ on a good year and $x_{t+1}=
$0.25 x_t$ on a bad year.



A basic question about this model is "in the long-term is this population increasing or decreasing?" To address this question, we will think about the problem in three ways: two analytical and one numerical. For the first approach, we can "in an 'average' year how many individuals does an individual replace itself with?" **Try this for yourself!** 

**Answer:** We have 30% of years individuals produce 0.25 offspring and 70% of years individuals produce 2 offspring. Hence, on average we get 
```{r}
0.4*0.25+0.6*2
```
offspring. Hence, one might expect that in the long-term that the population is increasing by 30% each year. Lets see if this works out in running numerical simulations. 

To run the simulations, we will do as you have done with Tim but need the added twist of randomly selecting whether it is a wet or dry year i.e. choose $0.25$ with probability $0.4$ and $2$ with probability $0.6$. One way to do this is with R's sample command. For example, the following code chooses $6$ random numbers this way by ''sampling'' six times from the vector x=c(0.25,2) with the corresponding probabilities in prob=c(0.4,0.6).

```{r}
sample(x=c(0.25,2),size = 6,replace = TRUE,prob = c(0.4,0.6))
```

To see if we do this enough we get the desired frequencies, lets sample 100,000 times and plot the histogram of resulting numbers. 

```{r}
output=sample(x=c(0.25,2),size = 100000,replace = TRUE,prob = c(0.4,0.6))
harry=hist(output,2)
harry$counts
```

OK looks good about 40% of one and 60% of the other. Now to run the dynamics, we can create a loop as you have done before and plot it.

```{r}
Time=100 # length of run
x=matrix(NA,Time,1) # column vector to hold output
x[1]=10000 # initial population size
for(t in 1:(Time-1)){
  R=sample(x=c(2,0.25),size=1,prob=c(0.6,0.4))
  x[t+1]=R*x[t]
}
plot(x,type="b")
```

**What do you notice?** To get a better "bead" on things, we could plot on a semi-log scale

```{r}
plot(x,type="b",log="y")
```

Hmm...looks like population is decreasing. Perhaps, we were unlucky with our run. Lets try running many copies of the model and plotting them together. This requires simple slightly modifying the previous code:


```{r}
Time=100 # length of run
reps=1000 # number of runs
x=matrix(NA,Time,reps) # matrix to hold output; columns are different runs
x[1,]=10000 # initial population size
for(t in 1:(Time-1)){
  R=sample(x=c(2,0.25),size=reps,replace=TRUE,prob=c(0.6,0.4))
  x[t+1,]=R*x[t,]
}
matplot(log(x),type="l")
```

Still looks like populations are tending to get smaller. Hmm....why is that? To get at the answer, we will take our third approach to thinking about the dynamics. Let $R(t+1)$ be the number of offspring per parent that make it to the $t+1$-th year i.e. $2$ or $0.25$ for our specific model. Our model can be rewritten as 
\[
x(t+1)=R(t+1)x(t)
\]
Iterating from $t=0$ gives
\[
\begin{aligned}
x(1)&=R(1)x(0)\\
x(2)&=R(2)x(1)=R(2)R(1)x(0)\\
x(3)&=R(3)x(2)=R(3)R(2)R(1)x(0)\\
\vdots & \\
x(t)&R(t)x(t-1)=R(t)R(t-1)...R(1)x(0)
\end{aligned}
\]
Now suppose we have $x(0)=10000$ and we are interested in $x(100)$ i.e. $t=100$. During those hundred years, we expect about $40$ dry years and $60$ wet years. Hence, 
\[
R(100)....R(1)\approx 0.25^{40}\times 2^{60}
\]
Equivalently
\[
0.25^{100\times 0.4} 2^{100\times 0.6}= \left(0.25^{0.4}\times 2^{0.6}\right)^100
\]
Hence, we expect 
\[
x(100)\approx \left(0.25^{0.4}\times 2^{0.6}\right)^100 x(0)
\]
more generally
\[
x(t)\approx \left(0.25^{0.4}\times 2^{0.6}\right)^t x(0)
\]
So what does this mean? Well 
```{r}
0.25^0.4*2^0.6
```
which is $<1$. Hence, the population should be decreasing in the long term. ***Write down an expression for $\log x(t)$ and plot it on our earlier plot***

***Answer:*** We have 
\[
\log x(t)=t(0.6 \log 2 - 0.4 \log 4)+\log x(0)
\]
Hence we should plot a line with slope $0.6 \log 2 - 0.4 \log 4$ and intercept $\log 10000$. 

```{r}
matplot(log(x),type="l")
abline(a=log(10000),b=0.6*log(2)-0.4*log(4),lwd=3,lty=2)
```

Hence, the quantity $0.25^{0.4}\times 2^{0.6}$ (or its log) seems to do a much better job of identifying how the population size is changing over time. This quantity is called the *geometric mean* of numbers $0.25,2$ which occur with probability $0.4,0.6$. More generally, given numbers $a_1,a_2,\dots,a_k$ that occur with likelihoods $p_1,p_2,\dots,p_k$, their geometric mean is given by 
\[
a_1^{p_1}a_2^{p_2}\dots a_k^{p_k}=:\prod_i a_i^{p_i}
\]
If these are the values that $R(t)$ take in our population model, we have the following observation: *If the geometric mean of the $R(t)$ is less than one, then population tends to toward extinction. If it is greater than one, then it grows exponentially.*  

Notice that for our special case the expected value (arithmetic mean) of $R(t)$ is less than the geometric mean. This holds generally. To get a better feel for this, **consider the following three scenarios**

1. $R(t)$ is $1.1$ every year
2. $R(t)$ is $1.3$ half of the years, and $0.9$ else.
3. $R(t)$ is $1.8$ half of the years, and $0.4$ else. 

**Find the arthemitic and geometric means for each of these scenarios. What does this imply?**

**Answer:**
For scenario 1, both means are 1.1

For scenario 2, the means are
```{r}
mean(c(1.3,0.9))
sqrt(1.3*0.9)
```

For scenario 3, the means are 
```{r}
mean(c(1.8,0.4))
sqrt(1.8*0.4)
```

So we see that arithmetic means are the same in all three scenarios despite the increasing variation in the $R(t)$ values. This increasing variation, however, leads to lower and lower geometric means where in the final case the population would be decreasing instead of increasing. 

## Bet hedging

The previous work suggests an intriguing possibility. Namely, as decreasing the variation in reproductive success can increase the geometric mean, *would evolution ever select for decreasing the variation in fitness at the expense of decreasing the arthematic mean in fitness?* We will see the answer is **yes!!!** and when this occurs, biologists (motivated by the economic analogy) say that **bet-hedging** has occurred. 

To see how this might arise, lets consider the case of seed banks in annual plants. Ecologists have noticed that the seeds of annual plants may not all germinate in a given year, but some may germinate one, two, or more years later. This occurs despite the seeds in a given region experiencing the same environmental conditions. This raises the question *Why would only some seeds germinate in one year and others in a later year?* 

Lets tackle this question with a modification of our earlier model. In this model, $x(t)$ will be the number of *seeds* in the population in year $t$ immediately prior to germination. Lets assume that a fraction $g$ of seeds germinate in any given year and the remaining fraction $1-g$ remain in the "seed bank". For each seed germinating in year $t$, it produces $Y(t+1)$ seeds surviving to the next year. For seed not germinating, it survives to the next year (e.g. escapes seed predation) with probability $s$. Armed with this information, **write down a model for the seed bank dynamics $x(t)$**.

**Answer:** The total number of seeds germinating is $gx(t)$. Each of these seeds contributes $Y(t+1)$ seeds to the seed bank in the next year. Hence, the net contribution of germinating seeds is $Y(t+1)gx(t)$. The total number of seeds not germinating is $(1-g)x(t)$. Each of these seeds survives with probability $s$ to the next year. Hence, the net contribution of non-germinating seeds is $s g x(t)$. Therefore, our model is given by 
\[
x(t+1)=Y(t+1)gx(t)+s(1-g)x(t)=(Y(t+1)g+s(1-g))x(t)
\]

**Can you think of a monetary analog of this model?** 

**Answer:** Let $x(t)$ be present value of  the money that you have in a savings account. Since the interest rate in a savings account is less than the inflation rate, the present value is decreasing slowly but steadly by $1-s$. Now, let $g$ be the fraction of money that you invest into a risky stock. Then $Y(t+1)$ is the amount of return you get from the stock after a year. 

For initial explorations, lets puts some specific numbers on this model. Lets say $Y(t)$ is $2$ in wet years and $0.25$ in dry years. 60% of the years are wet. Lets say seed survivorship is quite high, say $s=0.95$. Two questions: **What happens to the population in the long-term if all seeds germinate i.e. $g=1$? What happens if no seeds germiante i.e. $g=0$?**

**Answers:** If all seeds germinate, then we get the model from earlier and the population declines to extinction as the geometric mean of $R(t)=Y(t)$ is less than one. If no seeds germinate, the population size in year $t$ is $x(t)=0.95^5 x(0)$. Hence, it also declines to extinction. 

So it looks like always germinating or always not germinating are "loser" strategies. Are there any winning strategies for $g$? To answer this question, **find the choice of $g$ that maximizes the geometric mean of $R(t)=(gY(t)+(1-g)s)$.**

**Answer:** One can do this a variety of ways e.g. graphically, numerically testing different values, etc. Here, I will use the optimize command that we discussed in the module with Jay Rosenheim. First need to define the function being maximzied. We have in 40% of the years, $R(t)$ is equal to $2g+(1-g)0.95$ and $0.25g+(1-g)0.95$ in the remaining years. Hence, the geomtric mean of interest is given by

```{r}
f=function(g)(2*g+(1-g)*0.95)^0.6*(0.25*g+(1-g)*0.95)^0.4
```

We can plot this function with respect to $g$

```{r}
g=seq(0,1,length=50)
plot(g,f(g),type="l")
```

Looks like there is a maximum around $g=0.5$ which is $>1$. To find the exact value, we can use the optimize command.

```{r}
optimize(f,interval = c(0,1),maximum = TRUE)
```

So we get a maximum at $g=0.45$ of $1.03$. Hence, if the population had adopted this germination strategy of $45\%$ germination rate, then it would be growing by $3\%$ per year in the long-term. So there is a winner strategy!!! How does this winner strategy depend on the details? To answer this question, **examine how the optimal germination strategy and the growth rate of the corresponding population depends on seed survivorship, frequency of wet years, yield on wet years, yield on dry years**


**Answer:** Discussion about what each table found. 

Conclude by discussing the amazing work of Larry Venable! and the implications for making money.  